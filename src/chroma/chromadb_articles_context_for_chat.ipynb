{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H2Aax2OYKyhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dcdef95-5f9e-4726-dcf4-7296cda5e982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install chromadb langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "Tb4v2oD_L1C6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and split documents"
      ],
      "metadata": {
        "id": "ytIU6ZauRCi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "7vRO8-h2K8sX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q sample_data/articles.zip -d sample_data/import"
      ],
      "metadata": {
        "id": "PkautuDLOI4z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader(\"./sample_data/import/articles\", glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "FSQwoq0eN03D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents) # we only have 10 docuemnts"
      ],
      "metadata": {
        "id": "81yYPcklRohz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7feba042-d019-4daf-9503-afe6ed1fddce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[3].metadata"
      ],
      "metadata": {
        "id": "955GVbg5OgGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c450cbc-cf2f-4d5c-e660-0212beb770d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'sample_data/import/articles/bad_speeling.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "H2WgUTFDQQDm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "id": "r_qLkiv3RL7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c81141-ffe7-4032-ce59-193169a95842"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use chromadb collections"
      ],
      "metadata": {
        "id": "HEiL0dlNSQLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0].page_content"
      ],
      "metadata": {
        "id": "9Rb7Ts9YSDav",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5dfc65f0-f3cb-47b0-86c3-381897965831"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OpenAI may be synonymous with machine learning now and Google is doing its best to pick itself up off the floor, but both may soon face a new threat: rapidly multiplying open source projects that push the state of the art and leave the deep-pocketed but unwieldy corporations in their dust. This Zerg-like threat may not be an existential one, but it will certainly keep the dominant players on the defensive.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb"
      ],
      "metadata": {
        "id": "-xkX_4aNc_4R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.Client() # default in memory\n",
        "collection = client.create_collection('articles')"
      ],
      "metadata": {
        "id": "l9KUdoRXTvdf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.add(\n",
        "    ids=[str(i) for i in range(0, len(chunks))],\n",
        "    documents=[c.page_content for c in chunks],\n",
        "    metadatas=[c.metadata for c in chunks],\n",
        ") # using chroma build in embedding all-MiniLM-L6-v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF8OkR51T-rF",
        "outputId": "73191dbd-5f8c-4a35-c6f0-49e1d9ad40f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:06<00:00, 12.1MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what is Cherry'\n",
        "collection.query(\n",
        "    query_texts=query,\n",
        "    n_results=2)"
      ],
      "metadata": {
        "id": "UhtvLZJ1UYav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d7f3d0-baf1-4822-b063-7a3e3bdc7379"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': [['20', '15']],\n",
              " 'distances': [[0.6593900918960571, 0.8436243534088135]],\n",
              " 'metadatas': [[{'source': 'sample_data/import/articles/cherry.txt'},\n",
              "   {'source': 'sample_data/import/articles/cherry.txt'}]],\n",
              " 'embeddings': None,\n",
              " 'documents': [['In terms of the future, Cherry is focused on expanding its user base, integrating with online retailers and introducing personalized shopping recommendations in order to “cherry-pick” the best products for its users. The company is also exploring opportunities to partner with brands for exclusive deals and promotions.\\n\\nCherry is available on iOS and Android.',\n",
              "   'Meet Cherry, an AI shopping assistant that helps you discover products using screenshots or images\\nAisha Malik@aiishamalik1 / 2:18 PM GMT+1•March 21, 2024\\n Comment\\nCherry app displayed on smartphone screens\\nImage Credits: Cherry\\nA new app from a startup called Cherry is aiming to transform the online shopping experience with its AI assistant that allows users to discover products across the internet using just a screenshot or image. Cherry helps you find products that you’ve come across while scrolling through social media or have seen in real life.']],\n",
              " 'uris': None,\n",
              " 'data': None}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use different embedding function\n"
      ],
      "metadata": {
        "id": "GqmTr54WUwTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "lqWmzStnVI9l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vectors = OpenAIEmbeddings(model=\"text-embedding-3-large\").embed_documents([c.page_content for c in chunks])"
      ],
      "metadata": {
        "id": "azRxSAG3U2f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7682f39-5741-42f2-f735-0e0a43fed9c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#client.delete_collection('articles_with_embeddings')"
      ],
      "metadata": {
        "id": "BY4BPbIUhV0K"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_embed = client.create_collection('articles_with_embeddings')"
      ],
      "metadata": {
        "id": "JqlK6YhAdm-R"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_embed.add(\n",
        "    embeddings=embedding_vectors,\n",
        "    ids=[str(i) for i in range(0, len(chunks))],\n",
        "    documents=[c.page_content for c in chunks],\n",
        "    metadatas=[c.metadata for c in chunks],\n",
        ")"
      ],
      "metadata": {
        "id": "PItTrp9Ddryi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what is Cherry'\n",
        "collection.query(\n",
        "    query_texts=query,\n",
        "    n_results=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v7_lENrdy6c",
        "outputId": "48c64223-02cd-48d3-efdc-359cb4a4407c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': [['20', '15']],\n",
              " 'distances': [[0.6593900918960571, 0.8436243534088135]],\n",
              " 'metadatas': [[{'source': 'sample_data/import/articles/cherry.txt'},\n",
              "   {'source': 'sample_data/import/articles/cherry.txt'}]],\n",
              " 'embeddings': None,\n",
              " 'documents': [['In terms of the future, Cherry is focused on expanding its user base, integrating with online retailers and introducing personalized shopping recommendations in order to “cherry-pick” the best products for its users. The company is also exploring opportunities to partner with brands for exclusive deals and promotions.\\n\\nCherry is available on iOS and Android.',\n",
              "   'Meet Cherry, an AI shopping assistant that helps you discover products using screenshots or images\\nAisha Malik@aiishamalik1 / 2:18 PM GMT+1•March 21, 2024\\n Comment\\nCherry app displayed on smartphone screens\\nImage Credits: Cherry\\nA new app from a startup called Cherry is aiming to transform the online shopping experience with its AI assistant that allows users to discover products across the internet using just a screenshot or image. Cherry helps you find products that you’ve come across while scrolling through social media or have seen in real life.']],\n",
              " 'uris': None,\n",
              " 'data': None}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with langchain Chroma"
      ],
      "metadata": {
        "id": "DKavHguRiLBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "# langchain offers different vector dbs https://python.langchain.com/docs/modules/data_connection/vectorstores/"
      ],
      "metadata": {
        "id": "WXUIbFZVi-0l"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_function = OpenAIEmbeddings() # use default"
      ],
      "metadata": {
        "id": "IuGAikAsfSOW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = Chroma.from_documents(\n",
        "    documents = chunks,\n",
        "    embedding = embedding_function, # embedding function\n",
        "    persist_directory = 'storage' # saves as sqlite3 into folder storage\n",
        ")"
      ],
      "metadata": {
        "id": "pbAN2KJYiu0Y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.similarity_search(query='What is a Cherry', k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_56ZiC0jald",
        "outputId": "b8521876-8578-4226-ef20-d45838885278"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Meet Cherry, an AI shopping assistant that helps you discover products using screenshots or images\\nAisha Malik@aiishamalik1 / 2:18 PM GMT+1•March 21, 2024\\n Comment\\nCherry app displayed on smartphone screens\\nImage Credits: Cherry\\nA new app from a startup called Cherry is aiming to transform the online shopping experience with its AI assistant that allows users to discover products across the internet using just a screenshot or image. Cherry helps you find products that you’ve come across while scrolling through social media or have seen in real life.', metadata={'source': 'sample_data/import/articles/cherry.txt'}),\n",
              " Document(page_content='In terms of the future, Cherry is focused on expanding its user base, integrating with online retailers and introducing personalized shopping recommendations in order to “cherry-pick” the best products for its users. The company is also exploring opportunities to partner with brands for exclusive deals and promotions.\\n\\nCherry is available on iOS and Android.', metadata={'source': 'sample_data/import/articles/cherry.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector storage as retriever and chain with GPT\n",
        "\n",
        "use the vector db as document retriever and combine with LLM"
      ],
      "metadata": {
        "id": "XMV01A2bn_AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchainhub\n",
        "# pip install -U langchain langchain-community\n",
        "from langchain_community.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "N8hbJX7Zt5HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_db.as_retriever()"
      ],
      "metadata": {
        "id": "36akD6ERln4B"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.1,\n",
        "                 model_name=\"gpt-3.5-turbo\",\n",
        "                 api_key=userdata.get('openai_api_key'), # default is gpt-3.5 currently\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWyV9ND-oRlh",
        "outputId": "43fe030e-82f3-4776-bd10-8ed773e77f49"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.chat_models.openai:WARNING! search_kwargs is not default parameter.\n",
            "                    search_kwargs was transferred to model_kwargs.\n",
            "                    Please confirm that search_kwargs is what you intended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_community.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "63Pgv-V0qBpr"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "#https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html#langchain.chains.retrieval.create_retrieval_chain\n"
      ],
      "metadata": {
        "id": "PsMGc1RMp_v_"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html#langchain.chains.combine_documents.stuff.create_stuff_documents_chain\n",
        "# This chain takes a list of documents and formats them all into a prompt,\n",
        "# then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using."
      ],
      "metadata": {
        "id": "XG3tG15UuujG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with the help of llm creates natural language on inforamtion basis of the feeded documents (= context)\n",
        "prompt = retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
        "llm_promt_chain = create_stuff_documents_chain(\n",
        "    llm, prompt\n",
        ")\n",
        "vectordb_llm_chain = create_retrieval_chain(retriever, llm_promt_chain)\n",
        "\n",
        "result = vectordb_llm_chain.invoke({\"input\": \"What is a Cherry\"})"
      ],
      "metadata": {
        "id": "scZiopjGq0gd"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.get('answer')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dGSHZfTasZUW",
        "outputId": "8c45bd7e-9ba4-4316-e8d5-b7a8540d800f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cherry is an AI shopping assistant app developed by a startup that helps users discover products across the internet using screenshots or images. It allows users to find products they have seen on social media or in real life, sort results by price, bookmark products, and look at their image search history. Cherry is available on both iOS and Android platforms.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOJj-2pqreQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}